{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predykcja energii produkowanej przez panele słoneczne\n",
    "\n",
    "Niniejszy notebook stanowi podsumowanie prac nad predykcją energii produkowanej przez panele słoneczne w oparciu o dane ze strony projektu https://www.kaggle.com/c/solarscholar/data.\n",
    "\n",
    "## Opis\n",
    "\n",
    "Celem projektu była predykcja energii za pomocą wybranego przez siebie modelu regresji. Całość miała być wykonana z użyciem Pythona w oparciu o biblioteki _pandas_ oraz _sci-kit_. Wyjście programu miał stanowić plik .csv zawierający parę _id_czujnika,predykcja_prod_energii_.\n",
    "\n",
    "## Działanie skryptu\n",
    "\n",
    "### Zaimportowanie niezbędnych bibliotek\n",
    "\n",
    "Na początku należało zaimportować niezbędne biblioteki. Oprócz standardowych przyda się również _csv_ do obsługi plików .csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import preprocessing\n",
    "import csv as csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import i obróbka danych\n",
    "\n",
    "Ponieważ dostarczone z projektem pliki miały formę pliku .csv, wystarczyło użyć wbudowanej funkcji _read_csv_ z biblioteki _pandas_. Otwieramy zarówno zbiór treningowy, jak i testowy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_df = pd.read_csv(\"train.csv\", sep=\",\")\n",
    "test_df = pd.read_csv('test.csv', sep=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas na obróbkę danych. Zbiór wartości wyjściowej (kwh) przyda nam się w procesie tworzenia modelu, z kolei zbiór id będzie potrzebny w trakcie tworzenia wynikowego pliku .csv. Obie struktury zapisujemy w dedykowanych dla nich zmiennych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value_to_predict = np.array(X_df['kwh'])\n",
    "ids = test_df['id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Czas na usunięcie zbędnych kolumn. Robimy to z uwagi na ich niekompatybilny typ (data) lub ich znaczenie (pcnm*). Całość została opisana w poprzednim projekcie, wiec nie będziemy się zajmować nimi w tym momencie. Usuwamy wartość wyjściową (kwh) ze zbioru treningowego (zbiór testowy ich nie zawiera). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# oddzielnie usuwamy pcnm*, oddzielnie resztę\n",
    "non_cat_cols = [col for col in X_df.columns if 'pcnm' not in col]\n",
    "non_cat_cols_test = [col for col in test_df.columns if 'pcnm' not in col]\n",
    "X_df = X_df[non_cat_cols]\n",
    "test_df = test_df[non_cat_cols_test]\n",
    "\n",
    "X_df = X_df.drop(['id', 'data', 'icon', 'idmodel', 'idbrand', 'kwh'], axis=1)\n",
    "test_df = test_df.drop(['id', 'data', 'icon', 'idmodel', 'idbrand'], axis=1)\n",
    "x_train = np.array(X_df)\n",
    "x_test = np.array(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodatkowo, dane zostaną poddane skalowaniu i centrowaniu w oparciu o wbudowaną w bibliotekę _sci-kit_ klasę _StandardScaler_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tworzenie modelu predykcji\n",
    "\n",
    "Biblioteka _sci-kit_ umożliwia proste tworzenie modelu regresji. W naszym przypadku użyjemy regresora SGD (Stochastic Gradient Descent) stanowiącą rozwinięcie modelu regresji liniowej. Dodatkowo stosujemy w tym miejscu sieć elastyczną (ang. elastic net) łączącą regresję grzbietową z lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(penalty='elasticnet', max_iter=1000, tol=1e-3)\n",
    "sgd = sgd.fit(x_train, value_to_predict)\n",
    "output = sgd.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapis predykcji do pliku .csv\n",
    "\n",
    "Wynik predykcji zapisujemy do pliku za pomocą biblioteki _csv_, zgodnie z formatem zadanym na stronie projektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_file = open(\"predict.csv\", \"w\", newline=\"\")\n",
    "open_file_object = csv.writer(predictions_file)\n",
    "open_file_object.writerow([\"id\", \"kwh\"])\n",
    "open_file_object.writerows(list(zip(ids, abs(output))))\n",
    "predictions_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
